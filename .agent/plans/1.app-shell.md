# Module 1: App Shell — Auth, Chat UI, SSE Streaming

> **Complexity**: ⚠️ Medium — Two runtimes (Python + React), SSE wiring, JWT auth chain

## Goal

Stand up the complete end-to-end pipeline: user clicks the AI Tutor button → panel opens → types a message → message sent to FastAPI backend via SSE → response streamed back token-by-token → displayed in the chat panel. No RAG retrieval yet — just direct LLM chat with role-aware system prompts.

---

## Prerequisites

- [ ] Enable pgvector extension in Supabase dashboard (Database → Extensions → search "vector" → Enable)
- [ ] Note the `SUPABASE_JWT_SECRET` from Supabase dashboard (Settings → API → JWT Secret)
- [ ] Decide on initial LLM provider: OpenAI API key OR LM Studio running locally

---

## Tasks

### Task 1: Python Backend Scaffold

**Create `ai-tutor-api/` with FastAPI hello-world.**

Files to create:
```
ai-tutor-api/
├── pyproject.toml          # Project metadata, dependencies
├── requirements.txt        # Pinned deps for pip
├── .env.example            # Template for env vars
├── .gitignore              # venv/, __pycache__/, .env
├── src/
│   ├── __init__.py
│   ├── main.py             # FastAPI app, CORS, routes
│   ├── config.py           # Pydantic Settings (env vars)
│   └── auth.py             # JWT validation dependency
└── tests/
    ├── __init__.py
    └── test_health.py      # Smoke test for /health endpoint
```

**Dependencies** (requirements.txt):
```
fastapi>=0.115.0
uvicorn[standard]>=0.34.0
pyjwt>=2.9.0
python-dotenv>=1.0.1
httpx>=0.28.0
pydantic-settings>=2.7.0
sse-starlette>=2.2.0
openai>=1.60.0
supabase>=2.11.0
pytest>=8.3.0
pytest-asyncio>=0.24.0
```

**config.py** — Pydantic Settings:
```python
class Settings(BaseSettings):
    supabase_url: str
    supabase_service_role_key: str
    supabase_jwt_secret: str
    openai_api_key: str = ""
    openai_base_url: str = "https://api.openai.com/v1"  # Override for LM Studio
    openai_model: str = "gpt-4o-mini"
    cors_origins: list[str] = ["http://localhost:5173"]  # Vite dev server

    model_config = SettingsConfigDict(env_file=".env")
```

**main.py** — FastAPI app:
```python
app = FastAPI(title="Doorslam AI Tutor API")
app.add_middleware(CORSMiddleware, allow_origins=settings.cors_origins, ...)
app.include_router(chat_router, prefix="/chat")

@app.get("/health")
async def health():
    return {"status": "ok", "version": "0.1.0"}
```

**auth.py** — JWT dependency:
```python
async def get_current_user(authorization: str = Header(...)) -> dict:
    token = authorization.replace("Bearer ", "")
    payload = jwt.decode(token, settings.supabase_jwt_secret,
                         algorithms=["HS256"], audience="authenticated")
    return {"user_id": payload["sub"], "role": payload.get("role", "authenticated")}
```

**Validation**:
- [ ] `cd ai-tutor-api && python -m venv venv && source venv/bin/activate && pip install -r requirements.txt`
- [ ] `uvicorn src.main:app --reload` → starts on port 8000
- [ ] `curl http://localhost:8000/health` → `{"status": "ok", "version": "0.1.0"}`
- [ ] `pytest` → test_health passes

---

### Task 2: Vite Dev Proxy

**Modify `vite.config.ts`** — add server proxy so frontend requests to `/api/ai-tutor/*` reach FastAPI.

```typescript
server: {
  proxy: {
    '/api/ai-tutor': {
      target: 'http://localhost:8000',
      changeOrigin: true,
      rewrite: (path) => path.replace(/^\/api\/ai-tutor/, '')
    }
  }
}
```

**Add to `.env.example`**:
```
VITE_AI_TUTOR_API_URL=   # Leave empty for dev (uses Vite proxy). Set in production.
```

**Validation**:
- [ ] `npm run dev` running alongside `uvicorn`
- [ ] Browser: `http://localhost:5173/api/ai-tutor/health` → `{"status": "ok"}`

---

### Task 3: Chat API Endpoint (SSE)

**Create `ai-tutor-api/src/api/chat.py`** — POST `/chat/stream` endpoint.

Request body (Pydantic model in `src/models/chat.py`):
```python
class ChatRequest(BaseModel):
    message: str
    conversation_id: str | None = None
    role: Literal["parent", "child"] = "parent"
    child_id: str | None = None
    subject_id: str | None = None
    topic_id: str | None = None
```

Response: SSE stream with events:
```
event: token
data: {"content": "The"}

event: token
data: {"content": " answer"}

event: done
data: {"conversation_id": "uuid", "message_id": "uuid"}

event: error
data: {"error": "Something went wrong"}
```

**Implementation**:
1. Validate JWT via `Depends(get_current_user)`
2. Look up user role (parent or child) from Supabase `profiles`/`children` tables
3. Build system prompt based on role:
   - Parent: "You are an AI revision tutor helping a GCSE parent understand their child's subjects..."
   - Child: "You are a friendly study buddy helping a GCSE student with {subject}..."
4. Load conversation history from `rag.conversations`/`rag.messages` (or create new)
5. Call OpenAI completions API with streaming enabled
6. Yield SSE events as tokens arrive
7. After completion, save assistant message to `rag.messages`

**Pydantic models** (`src/models/chat.py`):
```python
class ChatRequest(BaseModel): ...
class ChatMessage(BaseModel):
    role: Literal["user", "assistant", "system"]
    content: str
class TokenEvent(BaseModel):
    content: str
class DoneEvent(BaseModel):
    conversation_id: str
    message_id: str
```

**Validation**:
- [ ] `curl -X POST http://localhost:8000/chat/stream -H "Authorization: Bearer <jwt>" -H "Content-Type: application/json" -d '{"message": "What is photosynthesis?"}'` → SSE stream of tokens
- [ ] New conversation created in `rag.conversations`
- [ ] Messages saved in `rag.messages`

---

### Task 4: RAG Schema Migration

**Create `supabase/migrations/YYYYMMDD_rag_schema.sql`**

This creates the `rag` schema with conversation tables (documents/chunks/embeddings come in Module 2):

```sql
-- Enable pgvector (must be enabled in dashboard first)
CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;

-- Create RAG schema
CREATE SCHEMA IF NOT EXISTS rag;

-- Conversations (parent or child chat threads)
CREATE TABLE rag.conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    child_id UUID REFERENCES public.children(id),
    subject_id UUID REFERENCES public.subjects(id),
    title TEXT,
    message_count INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT now(),
    last_active_at TIMESTAMPTZ DEFAULT now()
);

-- Messages within conversations
CREATE TABLE rag.messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID NOT NULL REFERENCES rag.conversations(id) ON DELETE CASCADE,
    role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    sources JSONB DEFAULT '[]'::jsonb,
    token_count INTEGER,
    model_name TEXT,
    latency_ms INTEGER,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- Indexes
CREATE INDEX idx_conversations_user ON rag.conversations(user_id);
CREATE INDEX idx_conversations_child ON rag.conversations(child_id);
CREATE INDEX idx_messages_conversation ON rag.messages(conversation_id);
CREATE INDEX idx_messages_created ON rag.messages(created_at);

-- RLS
ALTER TABLE rag.conversations ENABLE ROW LEVEL SECURITY;
ALTER TABLE rag.messages ENABLE ROW LEVEL SECURITY;

-- Users can manage their own conversations
CREATE POLICY "conversations_own" ON rag.conversations
    FOR ALL USING (user_id = auth.uid());

-- Parents can view their children's conversations
CREATE POLICY "conversations_parent_read" ON rag.conversations
    FOR SELECT USING (
        child_id IN (SELECT id FROM public.children WHERE parent_id = auth.uid())
    );

-- Messages visible via conversation ownership
CREATE POLICY "messages_own" ON rag.messages
    FOR ALL USING (
        conversation_id IN (SELECT id FROM rag.conversations WHERE user_id = auth.uid())
    );

-- Parents can view messages in their children's conversations
CREATE POLICY "messages_parent_read" ON rag.messages
    FOR SELECT USING (
        conversation_id IN (
            SELECT id FROM rag.conversations
            WHERE child_id IN (SELECT id FROM public.children WHERE parent_id = auth.uid())
        )
    );
```

**Validation**:
- [ ] Apply migration via Supabase SQL editor or `supabase db push`
- [ ] `SELECT * FROM rag.conversations;` returns empty result (no error)
- [ ] `SELECT * FROM rag.messages;` returns empty result (no error)
- [ ] RLS test: insert a conversation with a JWT, verify it's visible only to that user

---

### Task 5: Frontend Service Layer

**Create `src/services/aiAssistantService.ts`** — handles SSE streaming from the FastAPI backend.

```typescript
const AI_API = import.meta.env.VITE_AI_TUTOR_API_URL || '/api/ai-tutor';

interface ChatStreamOptions {
  message: string;
  conversationId?: string | null;
  role: 'parent' | 'child';
  childId?: string | null;
  subjectId?: string | null;
  topicId?: string | null;
  onToken: (content: string) => void;
  onDone: (data: { conversationId: string; messageId: string }) => void;
  onError: (error: string) => void;
}

export async function streamChat(options: ChatStreamOptions): Promise<void> {
  const { data: { session } } = await supabase.auth.getSession();
  if (!session?.access_token) {
    options.onError('Not authenticated');
    return;
  }

  const response = await fetch(`${AI_API}/chat/stream`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${session.access_token}`
    },
    body: JSON.stringify({
      message: options.message,
      conversation_id: options.conversationId,
      role: options.role,
      child_id: options.childId,
      subject_id: options.subjectId,
      topic_id: options.topicId
    })
  });

  if (!response.ok) {
    options.onError(`Request failed: ${response.status}`);
    return;
  }

  // Read SSE stream
  const reader = response.body?.getReader();
  const decoder = new TextDecoder();
  // ... parse SSE events, call onToken/onDone/onError callbacks
}
```

**Validation**:
- [ ] Import compiles: `npm run type-check` passes
- [ ] Service callable from a component (tested in Task 6)

---

### Task 6: Chat Panel UI

**Replace `src/components/layout/AiTutorSlot.tsx`** placeholder with a real chat interface.

**UI structure**:
```
┌─ Header ──────────────────────┐
│ ✨ AI Tutor        [New] [✕] │
├───────────────────────────────┤
│                               │
│  Message history              │
│  (scrollable)                 │
│                               │
│  ┌─ User bubble ────────────┐ │
│  │ What is photosynthesis?  │ │
│  └──────────────────────────┘ │
│                               │
│  ┌─ Assistant bubble ───────┐ │
│  │ Photosynthesis is the... │ │
│  │ ▌ (streaming cursor)     │ │
│  └──────────────────────────┘ │
│                               │
├───────────────────────────────┤
│ [Type a message...     ] [→] │
└───────────────────────────────┘
```

**State**:
```typescript
type PanelState = 'idle' | 'streaming' | 'error';

interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  createdAt: string;
}
```

**Key behaviours**:
- Messages scroll to bottom on new message
- Input disabled while streaming
- "New conversation" button clears messages and resets `conversationId`
- Error state shows retry button
- Uses `streamChat()` from `aiAssistantService.ts`
- Reads `isParent`/`isChild` from AuthContext to set the `role` parameter
- Reads `activeChildId` from AuthContext for child-scoped queries

**Reuse existing UI components**:
- `AppIcon` for sparkles, send, close, plus icons
- Design tokens from `themes.css` for colours
- Tailwind utilities consistent with existing panel styling

**Files**:
- `src/components/layout/AiTutorSlot.tsx` — rewrite (keep same file, replace placeholder)
- `src/components/layout/AiTutorSlot/MessageBubble.tsx` — new, message rendering
- `src/components/layout/AiTutorSlot/ChatInput.tsx` — new, text input + send button

**Validation**:
- [ ] Click AI Tutor button in sidebar → panel opens
- [ ] Type message → press enter or click send → message appears as user bubble
- [ ] Assistant response streams in token-by-token
- [ ] Auto-scroll follows streaming text
- [ ] "New" button resets conversation
- [ ] Close button closes panel
- [ ] `npm run lint` — zero warnings
- [ ] `npm run type-check` — passes
- [ ] `npm run build` — passes

---

### Task 7: Conversation Persistence

**Backend**: Save conversations and messages to `rag.conversations` and `rag.messages` tables.

- On first message in a new conversation: create `rag.conversations` row, return `conversation_id` in the `done` SSE event
- On subsequent messages: use existing `conversation_id`
- Save both user message and assistant response to `rag.messages`
- Track `message_count`, `last_active_at` on the conversation

**Frontend**: Persist `conversationId` in component state. On panel reopen, optionally load the most recent conversation.

**Validation**:
- [ ] Send a message → check `rag.conversations` has a new row
- [ ] Send a second message → same `conversation_id`, `message_count` = 4 (2 user + 2 assistant)
- [ ] Close and reopen panel → previous conversation loads (or starts fresh — decide UX)

---

## Integration Test

End-to-end flow:
1. Log in as a parent
2. Click AI Tutor button in sidebar → panel slides open
3. Type "What topics should my child focus on for GCSE Maths?" → press Enter
4. See assistant response stream in token-by-token
5. Type a follow-up → see conversational context maintained
6. Close panel → reopen → see the conversation history
7. Click "New" → start a fresh conversation
8. Log in as a child → panel shows child-appropriate system prompt behaviour

---

## Files Summary

| File | Action |
|------|--------|
| `ai-tutor-api/pyproject.toml` | New |
| `ai-tutor-api/requirements.txt` | New |
| `ai-tutor-api/.env.example` | New |
| `ai-tutor-api/.gitignore` | New |
| `ai-tutor-api/src/__init__.py` | New |
| `ai-tutor-api/src/main.py` | New |
| `ai-tutor-api/src/config.py` | New |
| `ai-tutor-api/src/auth.py` | New |
| `ai-tutor-api/src/api/__init__.py` | New |
| `ai-tutor-api/src/api/chat.py` | New |
| `ai-tutor-api/src/models/__init__.py` | New |
| `ai-tutor-api/src/models/chat.py` | New |
| `ai-tutor-api/tests/__init__.py` | New |
| `ai-tutor-api/tests/test_health.py` | New |
| `supabase/migrations/YYYYMMDD_rag_schema.sql` | New |
| `vite.config.ts` | Modify — add server proxy |
| `.env.example` | Modify — add VITE_AI_TUTOR_API_URL |
| `src/services/aiAssistantService.ts` | New |
| `src/components/layout/AiTutorSlot.tsx` | Rewrite |
| `src/components/layout/AiTutorSlot/MessageBubble.tsx` | New |
| `src/components/layout/AiTutorSlot/ChatInput.tsx` | New |
