# Module 2: BYO Retrieval + Memory

> **Complexity**: ⚠️ High — Batch ingestion pipeline, vector search, memory management, source citations
> **Branch**: `feature/ai-tutor-module2-retrieval`

## Context

Module 1 (App Shell) established the end-to-end chat pipeline: panel UI → SSE streaming → OpenAI → conversation persistence. Chat currently has **no access to revision content** — it answers purely from GPT's general knowledge.

Module 2 adds the RAG layer: programmatic ingestion of thousands of PDFs from a structured Google Drive into Supabase's pgvector store, retrieval-augmented chat responses with source citations, and conversation memory management.

**Critical design constraint**: Ingestion is entirely an admin/backend concern. Parents and children never see how content enters the system — they only see the improved chat responses with source citations. There is **no ingestion UI** in the main application.

---

## Google Drive Source Structure

Content lives in Google Drive with metadata encoded in the folder hierarchy:

```
<board>/                          → exam_boards.code (AQA, Edexcel, OCR)
  <qualification>/                → qualifications.code (GCSE, IGCSE)
    <Subject + code>/             → subjects.code (e.g., "1MA1")
      papers/
        <year>/                   → document metadata (2023, 2024)
          <file.pdf>              → source_type: 'past_paper'
      specs/
        <file.pdf>                → source_type: 'specification'
      revision/
        Sne/                      → source_type: 'revision', provider: 'seneca'
          <file.pdf>
        Pmt/                      → source_type: 'revision', provider: 'pmt'
          <file.pdf>
```

**Database mapping** (all entities already exist in `public` schema):
- `<board>` → `SELECT id FROM exam_boards WHERE code = $1`
- `<qualification>` → `SELECT id FROM qualifications WHERE code = $1`
- `<Subject + code>` → `SELECT id FROM subjects WHERE code = $1 AND exam_board_id = $2 AND qualification_id = $3`

---

## Parallelization Strategy

### What can run in parallel (4 independent tracks):

```
┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐
│  TRACK A: Database   │  │  TRACK B: Services   │  │  TRACK C: Frontend   │  │  TRACK D: Drive      │
│                      │  │                      │  │  Source Citations     │  │  Path Parser         │
│  • Migration SQL     │  │  • parser.py         │  │                      │  │                      │
│  • Tables + indexes  │  │  • chunker.py        │  │  • SourceChips.tsx   │  │  • drive_walker.py   │
│  • RLS policies      │  │  • embedder.py       │  │  • MessageBubble mod │  │  • path_parser.py    │
│  • search_chunks()   │  │  • memory.py         │  │  • SSE handler mod   │  │  • metadata resolver │
│  • Run migration     │  │  • Unit tests        │  │  • Types + tests     │  │  • Unit tests        │
└─────────┬───────────┘  └─────────┬───────────┘  └─────────┬───────────┘  └─────────┬───────────┘
          │                        │                         │                        │
          ▼                        ▼                         │                        ▼
┌─────────────────────────────────────────────┐              │    ┌──────────────────────────────┐
│  SEQUENTIAL: Ingestion Pipeline              │              │    │  (needs Track A + B + D)     │
│  • ingestion.py orchestrator                 │              │    └──────────────────────────────┘
│  • Batch processing with progress tracking   │              │
│  • CLI management script                     │              │
└─────────────────────┬───────────────────────┘              │
                      ▼                                       │
┌─────────────────────────────────────────────┐              │
│  SEQUENTIAL: Retrieval + Chat Integration    │              │
│  • retrieval.py service                      │              │
│  • Modify chat.py (context injection)        │              │
│  • Memory trimming in chat flow              │              │
│  • SSE sources event                         │◄─────────────┘
└─────────────────────┬───────────────────────┘  (frontend wired up here)
                      ▼
┌─────────────────────────────────────────────┐
│  SEQUENTIAL: End-to-end testing + docs       │
└──────────────────────────────────────────────┘
```

**Tracks A, B, C, D** can all be built simultaneously by parallel subagents.
**Ingestion pipeline** needs A + B + D complete.
**Retrieval + chat integration** needs A + ingestion (for test data).
**Frontend wiring** needs C + backend SSE format finalized.

---

## Prerequisites

- [ ] pgvector extension already enabled (done in Module 1)
- [ ] OpenAI API key with access to `text-embedding-3-small`
- [ ] Google Drive API credentials (service account JSON) for programmatic access
- [ ] Supabase dashboard access to run migration SQL
- [ ] Sample PDFs from the Drive for development/testing (don't need all thousands initially)

---

## Tasks

### Track A — Task 1: Database Migration

**Create `supabase/migrations/YYYYMMDD_rag_module2_documents.sql`**

**`rag.documents`** — Ingested file metadata:
```sql
CREATE TABLE rag.documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title TEXT NOT NULL,
    source_type TEXT NOT NULL CHECK (source_type IN (
        'past_paper', 'specification', 'revision', 'marking_scheme',
        'examiner_report', 'grade_threshold'
    )),
    source_path TEXT,                    -- Google Drive path or URL
    provider TEXT,                       -- 'seneca', 'pmt', null for official papers
    year INTEGER,                        -- exam year (for papers)
    subject_id UUID REFERENCES public.subjects(id),
    topic_id UUID REFERENCES public.topics(id),
    exam_board_id UUID REFERENCES public.exam_boards(id),
    qualification_id UUID REFERENCES public.qualifications(id),
    content_hash TEXT NOT NULL,          -- SHA-256 for deduplication
    chunk_count INTEGER DEFAULT 0,
    file_size INTEGER,
    status TEXT NOT NULL DEFAULT 'pending'
        CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
    error_message TEXT,
    metadata JSONB DEFAULT '{}'::jsonb,  -- page count, drive_file_id, etc.
    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now(),
    CONSTRAINT documents_content_hash_unique UNIQUE (content_hash)
);
```

**`rag.chunks`** — Text chunks with inline embeddings:
```sql
CREATE TABLE rag.chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES rag.documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    content_hash TEXT NOT NULL,
    token_count INTEGER,
    embedding vector(1536),
    -- Denormalized for fast pre-filtering during vector search
    subject_id UUID REFERENCES public.subjects(id),
    topic_id UUID REFERENCES public.topics(id),
    exam_board_id UUID REFERENCES public.exam_boards(id),
    metadata JSONB DEFAULT '{}'::jsonb,  -- page number, section heading
    created_at TIMESTAMPTZ DEFAULT now()
);
```

**`rag.ingestion_jobs`** — Batch progress tracking:
```sql
CREATE TABLE rag.ingestion_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    batch_label TEXT,                    -- e.g., "AQA/GCSE/1MA1" or "full-ingest-2026-02"
    status TEXT NOT NULL DEFAULT 'queued'
        CHECK (status IN ('queued', 'running', 'completed', 'failed', 'cancelled')),
    total_documents INTEGER DEFAULT 0,
    processed_documents INTEGER DEFAULT 0,
    failed_documents INTEGER DEFAULT 0,
    total_chunks INTEGER DEFAULT 0,
    error_log JSONB DEFAULT '[]'::jsonb, -- array of {document_id, error, timestamp}
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT now()
);
```

**`rag.search_chunks()`** — Postgres function for vector similarity search:
```sql
CREATE OR REPLACE FUNCTION rag.search_chunks(
    query_embedding vector(1536),
    match_count INTEGER DEFAULT 5,
    similarity_threshold FLOAT DEFAULT 0.7,
    filter_subject_id UUID DEFAULT NULL,
    filter_topic_id UUID DEFAULT NULL,
    filter_exam_board_id UUID DEFAULT NULL
) RETURNS TABLE (
    id UUID, document_id UUID, content TEXT, similarity FLOAT,
    document_title TEXT, source_type TEXT, subject_id UUID,
    topic_id UUID, chunk_metadata JSONB, doc_metadata JSONB
)
LANGUAGE plpgsql SECURITY DEFINER AS $$
BEGIN
    RETURN QUERY
    SELECT c.id, c.document_id, c.content,
           1 - (c.embedding <=> query_embedding) AS similarity,
           d.title, d.source_type, c.subject_id, c.topic_id,
           c.metadata, d.metadata
    FROM rag.chunks c
    JOIN rag.documents d ON d.id = c.document_id
    WHERE c.embedding IS NOT NULL
      AND d.status = 'completed'
      AND (filter_subject_id IS NULL OR c.subject_id = filter_subject_id)
      AND (filter_topic_id IS NULL OR c.topic_id = filter_topic_id)
      AND (filter_exam_board_id IS NULL OR c.exam_board_id = filter_exam_board_id)
      AND 1 - (c.embedding <=> query_embedding) > similarity_threshold
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END; $$;
```

**Indexes**:
```sql
-- Vector similarity (IVFFlat — adequate for <100K chunks, less memory than HNSW)
CREATE INDEX idx_chunks_embedding ON rag.chunks
    USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Pre-filter indexes for scoped retrieval
CREATE INDEX idx_chunks_subject ON rag.chunks(subject_id);
CREATE INDEX idx_chunks_topic ON rag.chunks(topic_id);
CREATE INDEX idx_chunks_exam_board ON rag.chunks(exam_board_id);
CREATE INDEX idx_chunks_document ON rag.chunks(document_id);

-- Document lookups
CREATE INDEX idx_documents_content_hash ON rag.documents(content_hash);
CREATE INDEX idx_documents_status ON rag.documents(status);
CREATE INDEX idx_documents_subject ON rag.documents(subject_id);

-- Ingestion job tracking
CREATE INDEX idx_ingestion_jobs_status ON rag.ingestion_jobs(status);
```

**RLS Policies**:
```sql
-- Documents/chunks: all authenticated can read, only service_role writes
ALTER TABLE rag.documents ENABLE ROW LEVEL SECURITY;
CREATE POLICY "documents_read" ON rag.documents FOR SELECT
    USING (auth.role() = 'authenticated');
CREATE POLICY "documents_service" ON rag.documents FOR ALL
    USING (auth.role() = 'service_role');

ALTER TABLE rag.chunks ENABLE ROW LEVEL SECURITY;
CREATE POLICY "chunks_read" ON rag.chunks FOR SELECT
    USING (auth.role() = 'authenticated');
CREATE POLICY "chunks_service" ON rag.chunks FOR ALL
    USING (auth.role() = 'service_role');

-- Ingestion jobs: service_role only
ALTER TABLE rag.ingestion_jobs ENABLE ROW LEVEL SECURITY;
CREATE POLICY "jobs_service" ON rag.ingestion_jobs FOR ALL
    USING (auth.role() = 'service_role');
```

**Permissions** (same pattern as Module 1 migration):
```sql
GRANT ALL ON rag.documents TO service_role;
GRANT SELECT ON rag.documents TO authenticated;
GRANT ALL ON rag.chunks TO service_role;
GRANT SELECT ON rag.chunks TO authenticated;
GRANT ALL ON rag.ingestion_jobs TO service_role;
```

**Validation**:
- [ ] Run migration via Supabase SQL editor
- [ ] Verify tables: `SELECT count(*) FROM rag.documents;`
- [ ] Verify function: `SELECT * FROM rag.search_chunks(NULL::vector(1536));`

---

### Track B — Task 2: Pure Backend Services

Create `ai-tutor-api/src/services/` with independent, testable modules.

**New dependencies** (`requirements.txt` additions):
| Package | Purpose |
|---------|---------|
| `tiktoken>=0.7.0` | Token counting for chunking + memory |
| `pymupdf>=1.24.0` | PDF text extraction |
| `python-docx>=1.1.0` | DOCX text extraction |
| `tenacity>=9.0.0` | Retry logic for API calls |

**`src/services/parser.py`** — Document text extraction:
- `parse_document(file_bytes: bytes, source_type: str) -> ParsedDocument`
- PDF via PyMuPDF, DOCX via python-docx, MD/TXT direct read
- Returns: `ParsedDocument(text, page_texts, page_count, metadata)`

**`src/services/chunker.py`** — Recursive character text splitting:
- `chunk_text(text, chunk_size=800, chunk_overlap=100) -> list[Chunk]`
- Token counting via tiktoken `cl100k_base`
- Separators: `["\n\n", "\n", ". ", " ", ""]`
- Returns `Chunk(content, index, token_count, content_hash)`

**`src/services/embedder.py`** — Batch embedding generation:
- `embed_chunks(texts: list[str]) -> list[list[float]]`
- OpenAI `text-embedding-3-small` via async client
- Batches of 100 per API call
- Retry with exponential backoff (tenacity, 3 retries)
- Configurable via Settings (model, base_url, dimensions)

**`src/services/memory.py`** — Conversation memory management:
- `trim_history(messages, max_tokens=4000) -> list[dict]`
- Walks backwards, accumulates within token budget
- Always preserves at least the last user message
- `count_tokens(text) -> int` helper

**Config additions** (`src/config.py`):
```python
# Embedding (separate from chat model — may use different provider)
embedding_model: str = "text-embedding-3-small"
embedding_base_url: str = "https://api.openai.com/v1"
embedding_api_key: str = ""       # falls back to openai_api_key if empty
embedding_dimensions: int = 1536

# Retrieval
retrieval_match_count: int = 5
retrieval_similarity_threshold: float = 0.7
max_history_tokens: int = 4000
```

**Unit tests** (no DB or API needed — pure functions):
- `tests/test_chunker.py` — sizes, overlap, edge cases
- `tests/test_memory.py` — token budgets, preservation of recent messages
- `tests/test_parser.py` — PDF/DOCX/MD with small fixture files

**Validation**:
- [ ] `./venv/bin/python -m pytest tests/test_chunker.py tests/test_memory.py -v` — all pass

---

### Track C — Task 3: Frontend Source Citations

The **only user-facing change** in Module 2. Parents and children see source citations on chat responses.

**Modify `src/services/aiAssistantService.ts`**:
- Add `SourceCitation` type:
  ```typescript
  interface SourceCitation {
    documentTitle: string;
    sourceType: string;
    similarity: number;
  }
  ```
- Add `sources?: SourceCitation[]` to the `Message` interface (used in AiTutorSlot)
- Handle new `sources` SSE event in `streamChat()`:
  ```
  event: sources
  data: {"sources": [{"document_title": "...", "source_type": "past_paper", "similarity": 0.89}]}
  ```
- Add `onSources?: (sources: SourceCitation[]) => void` to `ChatStreamOptions`

**Create `src/components/layout/ai-tutor/SourceChips.tsx`**:
- Compact pill/chip UI below assistant messages
- Shows document title + source type icon (paper, spec, revision)
- Shows first 2, expandable to see all
- Styled with existing Tailwind design tokens

**Modify `src/components/layout/ai-tutor/MessageBubble.tsx`**:
- Accept optional `sources?: SourceCitation[]` prop
- Render `<SourceChips>` below assistant bubble content

**Modify `src/components/layout/AiTutorSlot.tsx`**:
- Track sources per message in state
- Wire `onSources` callback to attach citations to current assistant message
- Optionally show "Searching revision materials..." during retrieval

**Tests**:
- `src/components/layout/ai-tutor/SourceChips.test.tsx`

**Validation**:
- [ ] `npm run lint && npm run type-check && npm run build` — all pass
- [ ] Source chips render correctly with mock data

---

### Track D — Task 4: Google Drive Path Parser + Walker

Programmatic traversal of the Google Drive folder structure to discover and download documents.

**`src/services/drive_walker.py`** — Google Drive API integration:
- Uses `google-api-python-client` with service account credentials
- `walk_drive(root_folder_id) -> list[DriveFile]`
- Recursively traverses folder hierarchy
- Returns flat list of files with full path metadata
- Respects rate limits (100 requests/second default)

**`src/services/path_parser.py`** — Extract metadata from folder paths:
- `parse_drive_path(path: str) -> DocumentMetadata`
- Parses: `AQA/GCSE/1MA1/papers/2024/June_Paper1.pdf`
- Returns:
  ```python
  @dataclass
  class DocumentMetadata:
      exam_board_code: str      # "AQA"
      qualification_code: str   # "GCSE"
      subject_code: str         # "1MA1"
      source_type: str          # "past_paper" | "specification" | "revision"
      provider: str | None      # "seneca" | "pmt" | None
      year: int | None          # 2024 (for papers)
      filename: str             # "June_Paper1.pdf"
  ```

**`src/services/metadata_resolver.py`** — Resolve path metadata to DB IDs:
- `resolve_metadata(meta: DocumentMetadata) -> ResolvedMetadata`
- Queries `public.exam_boards`, `public.qualifications`, `public.subjects`
- Maps codes to UUIDs: `exam_board_code → exam_board_id`, etc.
- Caches lookups (small number of boards/quals/subjects)
- Raises clear errors if a code doesn't match any DB record

**New dependency**: `google-api-python-client>=2.0.0` + `google-auth>=2.0.0`

**Unit tests**:
- `tests/test_path_parser.py` — various folder structure patterns, edge cases
- `tests/test_metadata_resolver.py` — mock Supabase queries, missing entities

**Validation**:
- [ ] Path parser correctly extracts metadata from all folder patterns
- [ ] Resolver maps codes to existing DB IDs
- [ ] Walker can list files from a test Drive folder

---

### Sequential — Task 5: Ingestion Pipeline Orchestrator

**Depends on**: Tracks A + B + D

**`src/services/ingestion.py`** — Orchestrates the full pipeline:
```python
async def ingest_document(
    file_bytes: bytes,
    metadata: ResolvedMetadata,
    source_path: str,
) -> str:
    """Parse → chunk → embed → store. Returns document_id."""
```

Pipeline per document:
1. Hash file bytes (SHA-256) → check for duplicate in `rag.documents`
2. Create document row (status: `processing`)
3. Parse document text (parser.py)
4. Chunk text (chunker.py)
5. Generate embeddings in batches (embedder.py)
6. Insert chunks with embeddings into `rag.chunks`
7. Update document (status: `completed`, chunk_count)
8. On error: set status `failed` with error_message

**`src/services/batch_ingestion.py`** — Batch processing for thousands of files:
```python
async def ingest_from_drive(
    root_folder_id: str,
    batch_label: str,
    concurrency: int = 5,
) -> str:
    """Walk Drive → download → ingest all files. Returns job_id."""
```

- Creates `rag.ingestion_jobs` row for the batch
- Uses `asyncio.Semaphore(concurrency)` to limit parallel processing
- Downloads files from Drive, pipes through `ingest_document()`
- Updates job progress (`processed_documents`, `failed_documents`)
- Logs errors to `error_log` JSONB array
- Skips duplicates (content_hash match)

**`src/api/ingestion.py`** — Admin-only API endpoints:
```
POST /ingestion/batch
  Body: { root_folder_id, batch_label?, concurrency? }
  Auth: service_role key (not regular JWT)
  Returns: { job_id }
  Behaviour: kicks off asyncio.create_task(ingest_from_drive(...))

GET /ingestion/jobs/{job_id}
  Returns: { status, total_documents, processed_documents, failed_documents, error_log }

GET /ingestion/documents
  Query: limit, offset, status?, subject_id?
  Returns: { documents, has_more }
```

**CLI management script** — `ai-tutor-api/scripts/ingest.py`:
```bash
# Ingest an entire Drive folder
./venv/bin/python scripts/ingest.py --folder-id <GOOGLE_DRIVE_FOLDER_ID> --label "AQA-GCSE-full"

# Ingest a specific subject
./venv/bin/python scripts/ingest.py --folder-id <FOLDER_ID> --label "AQA-GCSE-1MA1" --concurrency 10

# Check job status
./venv/bin/python scripts/ingest.py --status <JOB_ID>
```

**Validation**:
- [ ] Ingest 3-5 sample PDFs → documents + chunks + embeddings in DB
- [ ] Duplicate upload → skipped (content_hash match)
- [ ] Job progress updates correctly
- [ ] Failed document doesn't halt the batch

---

### Sequential — Task 6: Retrieval Service + Chat Integration

**Depends on**: Track A + Task 5 (needs data in DB to test)

**`src/services/retrieval.py`** — Vector search with role-based scoping:
```python
async def retrieve_context(
    query: str,
    subject_id: str | None,
    topic_id: str | None,
    role: str,
    user_id: str,
) -> list[RetrievedChunk]:
```

- Embeds the user query via `text-embedding-3-small`
- Calls `rag.search_chunks()` Postgres function with scope filters
- **Child**: filtered by `subject_id` + `topic_id` + resolved `exam_board_id`
- **Parent**: if `subject_id` provided, use it; otherwise search across all children's subjects (lookup via `child_subjects` table)
- Returns ranked chunks with similarity scores and document metadata

**Modify `src/api/chat.py`** — Core integration:

1. **Retrieve context** before building messages:
   ```python
   chunks = await retrieve_context(req.message, req.subject_id, req.topic_id, req.role, user["user_id"])
   ```

2. **Format + inject context** as second system message:
   ```python
   messages = [
       {"role": "system", "content": system_prompt},
       {"role": "system", "content": format_retrieval_context(chunks)},
       *trimmed_history,
       {"role": "user", "content": req.message},
   ]
   ```

3. **Apply memory trimming**:
   ```python
   raw_history = await _load_history(conversation_id)
   trimmed_history = trim_history(raw_history, max_tokens=settings.max_history_tokens)
   ```

4. **Send sources via SSE** (new event before `done`):
   ```
   event: sources
   data: {"sources": [{"document_title": "...", "source_type": "past_paper", "similarity": 0.89}]}
   ```

5. **Save sources** to `rag.messages.sources` JSONB on assistant message

**Decision — Always search (not tool calling)**:
Every student question is about curriculum content. Always embed+search (~50ms, $0.0001) is cheaper and more reliable than tool calling (extra LLM round-trip). If no chunks pass the 0.7 threshold, the context message says "No relevant materials found."

**Validation**:
- [ ] Chat about an ingested topic → response references the content
- [ ] `rag.messages.sources` populated with chunk refs
- [ ] Chat about an un-ingested topic → general knowledge, no sources
- [ ] Long conversation → older messages trimmed, quality stable

---

### Sequential — Task 7: Wire Frontend to Backend

**Depends on**: Track C (frontend components built) + Task 6 (SSE format finalized)

- Connect the `SourceChips` component to live SSE `sources` events
- Verify end-to-end: chat → retrieval → sources displayed
- Handle edge cases: no sources, streaming errors

**Validation**:
- [ ] Source chips appear below assistant messages with document titles
- [ ] No sources → no chips shown (clean UI)
- [ ] `npm run lint && npm run type-check && npm run test && npm run build`

---

### Sequential — Task 8: Tests + Documentation

**Backend tests**:
- `tests/test_ingestion_api.py` — batch endpoint, job status, dedup
- `tests/test_retrieval.py` — scoping logic, empty results, threshold
- `tests/test_path_parser.py` — folder structure patterns

**Frontend tests**:
- `src/components/layout/ai-tutor/SourceChips.test.tsx`

**Documentation**:
- `CHANGELOG.md` — entry under `[Unreleased]` → `### Added`
- `docs/PRODUCT_EVOLUTION.md` — Module 2 section (Why/What/How)
- No new ADR (ADR-007 covers the architecture)

**Full CI check**:
- [ ] `cd ai-tutor-api && ./venv/bin/python -m pytest tests/ -v`
- [ ] `npm run lint && npm run type-check && npm run test && npm run build`

---

## Files Summary

### New Files

| File | Track | Purpose |
|------|-------|---------|
| `supabase/migrations/YYYYMMDD_rag_module2.sql` | A | Documents, chunks, jobs tables + search function |
| `ai-tutor-api/src/services/__init__.py` | B | Package init |
| `ai-tutor-api/src/services/parser.py` | B | PDF/DOCX/MD text extraction |
| `ai-tutor-api/src/services/chunker.py` | B | Recursive text splitting |
| `ai-tutor-api/src/services/embedder.py` | B | Batch embedding generation |
| `ai-tutor-api/src/services/memory.py` | B | Conversation memory trimming |
| `src/components/layout/ai-tutor/SourceChips.tsx` | C | Citation chip UI component |
| `ai-tutor-api/src/services/drive_walker.py` | D | Google Drive API traversal |
| `ai-tutor-api/src/services/path_parser.py` | D | Folder path → metadata extraction |
| `ai-tutor-api/src/services/metadata_resolver.py` | D | Metadata codes → DB UUIDs |
| `ai-tutor-api/src/services/ingestion.py` | 5 | Per-document ingestion pipeline |
| `ai-tutor-api/src/services/batch_ingestion.py` | 5 | Batch processing orchestrator |
| `ai-tutor-api/src/api/ingestion.py` | 5 | Admin-only API endpoints |
| `ai-tutor-api/src/models/ingestion.py` | 5 | Pydantic models for ingestion |
| `ai-tutor-api/src/services/retrieval.py` | 6 | Vector search + role scoping |
| `ai-tutor-api/src/models/retrieval.py` | 6 | Pydantic models for retrieval |
| `ai-tutor-api/scripts/ingest.py` | 5 | CLI management script |
| `ai-tutor-api/tests/test_chunker.py` | B | Unit tests |
| `ai-tutor-api/tests/test_memory.py` | B | Unit tests |
| `ai-tutor-api/tests/test_parser.py` | B | Unit tests |
| `ai-tutor-api/tests/test_path_parser.py` | D | Unit tests |
| `ai-tutor-api/tests/test_ingestion_api.py` | 8 | API tests |
| `ai-tutor-api/tests/test_retrieval.py` | 8 | Retrieval tests |
| `src/components/layout/ai-tutor/SourceChips.test.tsx` | C | Frontend tests |

### Modified Files

| File | Task | Changes |
|------|------|---------|
| `ai-tutor-api/requirements.txt` | B/D | Add tiktoken, pymupdf, python-docx, tenacity, google-api-python-client, google-auth |
| `ai-tutor-api/src/config.py` | B | Add embedding + retrieval settings |
| `ai-tutor-api/.env.example` | B/D | Add embedding + Google Drive config vars |
| `ai-tutor-api/src/main.py` | 5 | Register ingestion router |
| `ai-tutor-api/src/api/chat.py` | 6 | Integrate retrieval + memory + sources SSE event |
| `ai-tutor-api/src/models/chat.py` | 6 | Add sources to response models |
| `src/services/aiAssistantService.ts` | C | Handle sources SSE event, new types |
| `src/components/layout/AiTutorSlot.tsx` | C | Sources state, wire onSources callback |
| `src/components/layout/ai-tutor/MessageBubble.tsx` | C | Render SourceChips |
| `CHANGELOG.md` | 8 | Module 2 entry |
| `docs/PRODUCT_EVOLUTION.md` | 8 | Module 2 section |

---

## Integration Test

1. Run CLI: `./venv/bin/python scripts/ingest.py --folder-id <TEST_FOLDER> --label "test-batch"`
2. Watch job progress: `--status <JOB_ID>` → queued → running → completed
3. Verify: `SELECT count(*) FROM rag.chunks WHERE embedding IS NOT NULL;` → chunks exist
4. Open AI Tutor as child → ask about ingested topic → response references content
5. See source chips below the assistant message
6. Ask follow-up → conversation history maintained
7. After 15+ exchanges → verify memory trimming working
8. Log in as parent → broader question → retrieval across children's subjects
9. Ask about un-ingested topic → general knowledge, no source chips

---

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Ingestion UI | None — CLI + API only | Admin concern, not user-facing. Thousands of PDFs need programmatic batch processing, not a UI |
| Google Drive integration | `google-api-python-client` with service account | Programmatic traversal of structured folder hierarchy |
| Metadata extraction | Parse from folder path | Folder structure encodes board/qual/subject/type/year — no manual tagging needed |
| Embeddings location | Column on chunks table | Avoids join on every retrieval query |
| Embedding model | text-embedding-3-small (1536d) | Cheapest, fast, good for educational content |
| Vector index | IVFFlat (100 lists) | Adequate for <100K chunks, less memory than HNSW |
| Chunking | Recursive character (800 tokens, 100 overlap) | Simple, debuggable, good for structured text |
| Batch processing | asyncio.Semaphore(concurrency) | No external infrastructure (Celery/Redis), same pattern as Module 1 |
| Retrieval | Always search (not tool calling) | Simpler, cheaper, more reliable for curriculum content |
| Memory | Sliding window (4000 tokens) | Simple, sufficient for focused GCSE conversations |
| Deduplication | SHA-256 content hash | DB-enforced uniqueness, skip already-ingested files |
