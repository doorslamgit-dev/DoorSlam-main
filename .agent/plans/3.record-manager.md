# Module 3: Record Manager — Change Detection & Incremental Sync

## Context

Module 2 built a complete RAG ingestion pipeline (Drive walker → parser → chunker → embedder → Supabase). It includes SHA-256 content hashing and duplicate detection at the file level — if identical bytes are uploaded twice, the second ingestion is skipped.

**What's missing**: When a file is *modified* on Drive (same file, new content), the system has no way to detect this. It would either skip it (if reuploaded with same bytes) or create a duplicate document row. There's also no mechanism to detect files removed from Drive, no incremental sync (process only what changed), and no cleanup of stale content.

**Goal**: Prevent duplicate ingestion and handle re-uploads cleanly:
- **Identical content** → skip processing entirely (return existing document)
- **Modified content** → delete old chunks, re-chunk, re-embed from scratch
- **Removed from Drive** → soft-delete, exclude from search immediately

---

## Tasks

### Task 1: Database Migration
**File**: `supabase/migrations/20260226120000_rag_module3_record_manager.sql` (NEW)

Add Drive identity tracking to `rag.documents`:
```sql
ALTER TABLE rag.documents
  ADD COLUMN drive_file_id TEXT,          -- Google Drive file ID (stable, immutable)
  ADD COLUMN drive_md5_checksum TEXT,     -- Drive-provided MD5 for fast change detection
  ADD COLUMN drive_modified_time TIMESTAMPTZ, -- Drive file last modified time
  ADD COLUMN deleted_at TIMESTAMPTZ;      -- Soft-delete timestamp
```

Constraints and indexes:
- `UNIQUE (drive_file_id)` — one document per Drive file (NULLs allowed for pre-Module 3 docs)
- Index on `drive_file_id` for sync lookups
- Expand status CHECK to include `'deleted'`

Extend `rag.ingestion_jobs` for sync:
- `job_type TEXT DEFAULT 'batch' CHECK (IN 'batch', 'sync')`
- `sync_stats JSONB` — `{added, updated, deleted, unchanged}` counts
- `root_folder_id TEXT` — which Drive folder was synced
- Index on `(root_folder_id, created_at DESC)` for "last sync" queries

---

### Task 2: Extend DriveFile Dataclass + Walker
**File**: `ai-tutor-api/src/services/drive_walker.py` (MODIFY)

- Add `md5_checksum: str | None = None` and `modified_time: str | None = None` to `DriveFile` dataclass (line 28)
- Update Drive API `fields` parameter (line 88) to request `md5Checksum, modifiedTime`
- Populate new fields when constructing `DriveFile` instances (lines 103-112)

Google Drive provides `md5Checksum` for all uploaded binary files (PDF, DOCX, TXT, MD) — exactly our supported MIME types.

---

### Task 3: Add `update_document()` and `soft_delete_document()` to Ingestion Service
**File**: `ai-tutor-api/src/services/ingestion.py` (MODIFY)

**`update_document(doc_id, file_bytes, filename, ...)`** — re-ingest modified content:
1. Set document status to `'processing'`
2. Delete all existing chunks for this document (`chunks.delete().eq("document_id", doc_id)`)
3. Recompute content_hash from new bytes
4. Parse → chunk → embed (reuse existing pipeline)
5. Insert new chunks
6. Update document row: new `content_hash`, `chunk_count`, `drive_md5_checksum`, `drive_modified_time`, `status='completed'`
7. **Preserve same document UUID** so message history references stay valid
8. Handle Storage: delete old file, upload new one (if `file_key` provided)

**`soft_delete_document(doc_id)`** — mark as deleted:
- Set `status='deleted'`, `deleted_at=now()`, `updated_at=now()`
- `search_chunks()` already filters `WHERE d.status = 'completed'` → excluded immediately

**`cleanup_deleted_documents(older_than_days=30)`** — hard-delete stale:
- Delete documents where `status='deleted' AND deleted_at < cutoff`
- `ON DELETE CASCADE` handles chunk removal automatically
- Return count of deleted documents

Also modify `ingest_document()` to accept optional `drive_file_id`, `drive_md5_checksum`, `drive_modified_time` params and store them on the document row.

---

### Task 4: Sync Orchestrator Service
**File**: `ai-tutor-api/src/services/sync.py` (NEW, ~200 lines)

Core function: `sync_from_drive(root_folder_id, batch_label, concurrency, root_path) → job_id`

**Algorithm**:
1. Create sync job row (`job_type='sync'`, `root_folder_id`)
2. Walk Drive folder → list of `DriveFile` (reuse `walk_drive()`)
3. Load existing documents from DB where `drive_file_id IS NOT NULL` → build dict `{drive_file_id: row}`
4. Classify each Drive file:
   - **New**: `file_id` not in DB → download, ingest (stores drive identity)
   - **Modified**: `file_id` in DB but `md5_checksum` differs → download, call `update_document()`
   - **Unchanged**: `md5_checksum` matches → skip
5. Detect deletions: DB documents whose `drive_file_id` not in current Drive file set → call `soft_delete_document()`
6. Process new/modified files with `asyncio.Semaphore(concurrency)`
7. Update job with `sync_stats: {added: N, updated: N, deleted: N, unchanged: N}`

---

### Task 5: Update Batch Ingestion to Store Drive Identity
**File**: `ai-tutor-api/src/services/batch_ingestion.py` (MODIFY, ~5 lines)

Pass Drive metadata through `process_file()` → `ingest_document()`:
```python
drive_file_id=drive_file.file_id,
drive_md5_checksum=drive_file.md5_checksum,
drive_modified_time=drive_file.modified_time,
```

This ensures all new batch ingestions also store Drive identity, so future syncs work without backfill.

---

### Task 6: API Endpoints + Pydantic Models
**Files**: `ai-tutor-api/src/api/ingestion.py` (MODIFY), `ai-tutor-api/src/models/ingestion.py` (MODIFY)

New endpoints (all require service_role key):

| Endpoint | Purpose |
|----------|---------|
| `POST /ingestion/sync` | Start incremental sync from Drive folder |
| `POST /ingestion/cleanup` | Hard-delete soft-deleted docs older than N days |

New Pydantic models: `SyncRequest`, `SyncResponse`, `CleanupRequest`, `CleanupResponse`

Extend `JobStatusResponse` with `job_type`, `sync_stats`, `root_folder_id`.

---

### Task 7: CLI Extensions
**File**: `ai-tutor-api/scripts/ingest.py` (MODIFY)

Add `--sync` and `--cleanup` subcommands:
```bash
./venv/bin/python scripts/ingest.py --sync --folder-id <ID> --label "nightly-sync"
./venv/bin/python scripts/ingest.py --cleanup --older-than 30
```

---

### Task 8: Tests
**New files**:
- `ai-tutor-api/tests/test_sync.py` — sync classification logic (new/modified/deleted/unchanged), sync stats
- `ai-tutor-api/tests/test_ingestion_api.py` — API endpoint tests (batch, sync, cleanup, job status)
- `ai-tutor-api/tests/test_retrieval.py` — retrieval service tests (search, format context)

All tests use existing mock patterns from `conftest.py` (MockQueryBuilder, monkeypatch).

---

### Task 9: Documentation
- `CHANGELOG.md` — `### Added` entry for Module 3
- `docs/PRODUCT_EVOLUTION.md` — new section explaining why/what/how
- No ADR needed (extends ADR-007, no new architectural decisions)

---

## Task Order

```
Task 1 (Migration)  ──────────────┐
Task 2 (DriveFile extension)  ────┤
                                  ▼
Task 3 (update/delete functions)  ──┐
Task 5 (batch stores drive ID)  ────┤
                                    ▼
Task 4 (sync orchestrator)  ────────┤
                                    ▼
Task 6 (API endpoints)  ───────────┤
Task 7 (CLI extensions)  ──────────┤
                                    ▼
Task 8 (tests)
Task 9 (documentation)
```

## Verification

1. **Unit tests**: `cd ai-tutor-api && ./venv/bin/python -m pytest tests/ -v`
2. **Lint + type-check + build**: `npm run lint && npm run type-check && npm run build`
3. **Manual sync test** (if Drive credentials configured):
   - Run batch ingest on a folder → verify `drive_file_id` stored
   - Run sync on same folder → verify all files classified as "unchanged"
   - Modify a file on Drive → run sync → verify old chunks deleted, new chunks created
   - Remove a file from Drive → run sync → verify soft-deleted, excluded from search
4. **Dedup verification**: Upload same file twice → second call returns existing doc ID

## Files Modified/Created

| File | Action |
|------|--------|
| `supabase/migrations/20260226120000_rag_module3_record_manager.sql` | NEW |
| `ai-tutor-api/src/services/drive_walker.py` | MODIFY |
| `ai-tutor-api/src/services/ingestion.py` | MODIFY |
| `ai-tutor-api/src/services/sync.py` | NEW |
| `ai-tutor-api/src/services/batch_ingestion.py` | MODIFY |
| `ai-tutor-api/src/api/ingestion.py` | MODIFY |
| `ai-tutor-api/src/models/ingestion.py` | MODIFY |
| `ai-tutor-api/scripts/ingest.py` | MODIFY |
| `ai-tutor-api/tests/test_sync.py` | NEW |
| `ai-tutor-api/tests/test_ingestion_api.py` | NEW |
| `ai-tutor-api/tests/test_retrieval.py` | NEW |
| `CHANGELOG.md` | MODIFY |
| `docs/PRODUCT_EVOLUTION.md` | MODIFY |
