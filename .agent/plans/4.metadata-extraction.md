# Module 4: Metadata Extraction — Implementation Plan

## Context

`topic_id` is NULL across all 110+ chunks in the RAG store. The `rag.search_chunks()` Postgres function already supports `filter_topic_id` and the chat API accepts it, but there's nothing to filter on. Module 4 adds LLM-based topic classification during ingestion so each chunk maps to the existing `public.topics` curriculum hierarchy, unlocking precise topic-scoped retrieval.

Module 3 (Record Manager) is in parallel — Module 4 only touches `topic_id` on chunks and adds new service files, so no conflicts.

**Branch**: `feature/ai-tutor-module4-metadata` from `develop`

---

## Curriculum hierarchy (already in DB)

```
public.subjects → public.components → public.themes → public.topics
                  (subject_id)        (component_id)   (theme_id, canonical_code, topic_name)
```

---

## Tasks

### Task 1: Config additions
**File**: `ai-tutor-api/src/config.py` (modify)

Add settings:
```python
extraction_model: str = "gpt-4o-mini"
extraction_temperature: float = 0.0
extraction_max_chunks_per_call: int = 10
extraction_confidence_threshold: float = 0.5
extraction_enabled: bool = True          # feature flag
```

Uses existing `openai_api_key` — no new env var needed.

---

### Task 2: Taxonomy loader
**File**: `ai-tutor-api/src/services/taxonomy.py` (new)

```python
@dataclass
class TopicEntry:
    topic_id: str
    topic_name: str
    canonical_code: str | None
    theme_name: str
    component_name: str

@dataclass
class SubjectTaxonomy:
    subject_id: str
    subject_name: str
    topics: list[TopicEntry]

@lru_cache(maxsize=32)
def load_taxonomy(subject_id: str) -> SubjectTaxonomy
def format_taxonomy_for_prompt(taxonomy: SubjectTaxonomy) -> str
```

Query chain: `components` (filter `subject_id`) → for each, fetch `themes` → for each, fetch `topics`. Cache result. Typically 50-200 topics per subject.

---

### Task 3: Metadata extractor service
**File**: `ai-tutor-api/src/services/metadata_extractor.py` (new)

```python
@dataclass
class TopicAssignment:
    topic_id: str
    topic_name: str
    confidence: float          # 0.0–1.0

@dataclass
class ChunkTopicResult:
    chunk_index: int
    primary_topic: TopicAssignment | None
    secondary_topics: list[TopicAssignment]  # max 2

async def extract_topics_for_chunks(
    chunks: list[tuple[int, str]],      # (index, content)
    taxonomy: SubjectTaxonomy,
    source_type: str,
    doc_title: str,
) -> list[ChunkTopicResult]
```

**LLM strategy**:
- Model: `gpt-4o-mini` (cheap, fast, good at structured classification)
- Response format: JSON mode
- Batches of ~10 chunks per call (taxonomy ~1K tokens + 10 × ~300 tokens)
- Prompt: provide numbered topic list, ask LLM to classify each chunk by topic number
- Confidence below threshold → `topic_id = NULL`
- Retry: 2 attempts with tenacity (matching `embedder.py` pattern)
- Failure: log warning, return NULL for all chunks — ingestion continues

**Cost**: ~$0.01 for 110 chunks, ~$1 for 10K chunks.

---

### Task 4: Ingestion pipeline integration
**File**: `ai-tutor-api/src/services/ingestion.py` (modify)

Insert extraction between embedding and chunk insertion. Run **in parallel** with embedding:

```python
# Current (lines 141-158):
embeddings = await embed_chunks(texts)
# ... insert chunks with topic_id=None

# New:
embed_task = asyncio.create_task(embed_chunks(texts))

topic_map: dict[int, str | None] = {}
if settings.extraction_enabled and subject_id:
    taxonomy = load_taxonomy(subject_id)
    if taxonomy.topics:
        results = await extract_topics_for_chunks(
            [(c.index, c.content) for c in chunks], taxonomy, source_type, title
        )
        for r in results:
            if r.primary_topic and r.primary_topic.confidence >= settings.extraction_confidence_threshold:
                topic_map[r.chunk_index] = r.primary_topic.topic_id

embeddings = await embed_task

# Insert chunks with per-chunk topic_id:
"topic_id": topic_map.get(i, topic_id),  # per-chunk or fallback
```

No changes to `batch_ingestion.py` — it already passes `subject_id` through.

---

### Task 5: Enhanced retrieval filters
**File**: `ai-tutor-api/src/services/retrieval.py` (modify)

Expose the filter params that `rag.search_chunks()` already supports but Python ignores:

```python
async def search_chunks(
    ...,
    source_type: str | None = None,   # NEW
    year: int | None = None,          # NEW
    exam_pathway_id: str | None = None,  # NEW
    doc_type: str | None = None,      # NEW
) -> list[RetrievedChunk]:
```

Pass them through to the RPC call. Also add `year`, `session`, `doc_type`, `file_key` to `RetrievedChunk` dataclass (returned by the Postgres function already but currently ignored).

Update `retrieve_context()` to accept and forward the same params.

**File**: `ai-tutor-api/src/models/chat.py` (modify — optional fields)

Add optional filter fields for future frontend use:
```python
class ChatRequest(BaseModel):
    ...
    doc_type: str | None = None
    year: int | None = None
    source_type: str | None = None
```

---

### Task 6: Backfill script
**File**: `ai-tutor-api/scripts/backfill_topics.py` (new)

```bash
cd ai-tutor-api && ./venv/bin/python scripts/backfill_topics.py
cd ai-tutor-api && ./venv/bin/python scripts/backfill_topics.py --subject-id <UUID>
cd ai-tutor-api && ./venv/bin/python scripts/backfill_topics.py --dry-run
```

1. Query `rag.chunks WHERE topic_id IS NULL`, grouped by `document_id`
2. For each document's chunks: load taxonomy, extract, UPDATE chunks
3. Idempotent — skips chunks that already have a `topic_id`
4. Progress counter, `--dry-run` flag

---

### Task 7: Tests
**New files**:
- `ai-tutor-api/tests/test_taxonomy.py` — load, format, empty subject, caching
- `ai-tutor-api/tests/test_metadata_extractor.py` — single chunk, batch, invalid JSON, out-of-range topic, API failure, below threshold, disabled flag, empty taxonomy

**Modified**: `ai-tutor-api/tests/conftest.py` — add mock taxonomy fixture + mock extraction LLM

Follow existing patterns: `MockQueryBuilder`, `AsyncMock` for OpenAI, pure function tests.

---

### Task 8: Documentation
- `CHANGELOG.md` — `### Added` entry for Module 4
- `docs/PRODUCT_EVOLUTION.md` — Module 4 section (Why/What/How)
- `docs/decisions/ADR-008-topic-extraction.md` — key decisions:
  1. Chunk-level extraction (not document-level) — past papers span many topics
  2. GPT-4o-mini — cost/speed tradeoff for classification
  3. Confidence threshold (0.5) — below = NULL
  4. Parallel embedding + extraction — latency optimization
  5. Feature flag — safe rollback

---

## File summary

| File | Action | Task |
|------|--------|------|
| `ai-tutor-api/src/config.py` | Modify | 1 |
| `ai-tutor-api/src/services/taxonomy.py` | New | 2 |
| `ai-tutor-api/src/services/metadata_extractor.py` | New | 3 |
| `ai-tutor-api/src/services/ingestion.py` | Modify | 4 |
| `ai-tutor-api/src/services/retrieval.py` | Modify | 5 |
| `ai-tutor-api/src/models/chat.py` | Modify | 5 |
| `ai-tutor-api/scripts/backfill_topics.py` | New | 6 |
| `ai-tutor-api/tests/test_taxonomy.py` | New | 7 |
| `ai-tutor-api/tests/test_metadata_extractor.py` | New | 7 |
| `ai-tutor-api/tests/conftest.py` | Modify | 7 |
| `CHANGELOG.md` | Modify | 8 |
| `docs/PRODUCT_EVOLUTION.md` | Modify | 8 |
| `docs/decisions/ADR-008-topic-extraction.md` | New | 8 |

---

## Verification

1. `cd ai-tutor-api && ./venv/bin/python -m pytest tests/ -v` — all pass
2. Backfill: `scripts/backfill_topics.py --subject-id <biology-uuid>` → chunks get `topic_id`
3. New ingestion: ingest a document → chunks have per-chunk `topic_id`
4. Feature flag off: `EXTRACTION_ENABLED=false` → ingestion succeeds with `topic_id=NULL`
5. Topic-scoped retrieval: chat with `topic_id` filter → only matching chunks returned
6. `npm run lint && npm run type-check && npm run test && npm run build` — frontend unaffected
