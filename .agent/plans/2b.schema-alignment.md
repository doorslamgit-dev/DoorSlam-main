# Plan: Schema Alignment + Ingestion Pipeline Enhancement

## Context

Module 2's RAG pipeline can parse, chunk, embed, and store documents — but it was built without full awareness of the existing database schema (`exam_spec_versions`, `exam_pathways`, `exam_papers`) and the highly structured naming conventions used in the Google Drive document collection.

Before testing ingestion with real documents, we need to:
1. **Extend `rag.documents`** with columns for session, paper number, tier, doc_type, spec version, and file storage key
2. **Create a Supabase Storage bucket** so original PDFs are downloadable by end users
3. **Build a filename parser** that extracts metadata from the standardized filenames (`{spec_code}_{year}_{session}_{paper}_{tier}_{type}.pdf`)
4. **Extend the metadata resolver** to look up `exam_spec_versions` and `exam_pathways`
5. **Update the search function** with new filter parameters
6. **Test with the AQA Biology (8461) specification** from Drive

All work on `feature/ai-tutor-module2-retrieval` branch (PR #46).

---

## Task 1: Database Migration

**File**: `supabase/migrations/20260225120000_rag_documents_schema_alignment.sql` (NEW)

### New columns on `rag.documents`

| Column | Type | FK | Purpose |
|--------|------|-----|---------|
| `exam_spec_version_id` | UUID | `public.exam_spec_versions(id)` | Which spec version this doc belongs to |
| `exam_pathway_id` | UUID | `public.exam_pathways(id)` | Foundation/higher/core/extended — maps tier to existing pathway model |
| `session` | TEXT | — | `jun`, `nov`, `mar` (from filename) |
| `paper_number` | TEXT | — | `p1`, `p2`, `p3` (from filename) |
| `doc_type` | TEXT | — | `qp`, `ms`, `er`, `gt`, `sp`, `spec`, `rev` (filename suffix) |
| `file_key` | TEXT | — | Supabase Storage path for the original PDF |

### Constraints

```sql
CHECK (session IS NULL OR session IN ('jun', 'nov', 'mar', 'jan'))
CHECK (doc_type IS NULL OR doc_type IN ('qp', 'ms', 'er', 'gt', 'sp', 'spec', 'rev'))
```

Expand `source_type` CHECK to include `'sample_paper'`.

### Indexes

```sql
idx_documents_year, idx_documents_doc_type, idx_documents_session,
idx_documents_exam_pathway, idx_documents_spec_version, idx_documents_source_type
```

### Updated `rag.search_chunks()`

Add optional filters: `filter_source_type`, `filter_year`, `filter_exam_pathway_id`, `filter_doc_type`.
Add return columns: `doc_year`, `doc_session`, `doc_paper_number`, `doc_type`, `doc_file_key`, `doc_exam_pathway_id`.

Re-GRANT execute permissions after DROP + CREATE.

### Run migration

```bash
export PATH="/opt/homebrew/bin:$PATH" && supabase db push --linked
```

---

## Task 2: Create Supabase Storage Bucket

**Bucket name**: `exam-documents`
- **Private** (not public) — copyrighted exam materials, access via signed URLs only
- **File size limit**: 50MB
- **Allowed MIME types**: `application/pdf`

**Storage path convention** (mirrors Drive, lowercase):
```
{board_code}/{qualification_code}/{spec_code}/spec/{filename}
{board_code}/{qualification_code}/{spec_code}/papers/{year}/{filename}
{board_code}/{qualification_code}/{spec_code}/revision/{provider}/{filename}
```

Example: `aqa/gcse/8461/papers/2024/8461_2024_jun_p1_higher_qp.pdf`

The `file_key` column in `rag.documents` stores this path. Download URL generated via:
```python
sb.storage.from_("exam-documents").create_signed_url(file_key, expires_in=300)
```

Create via Supabase CLI or dashboard.

---

## Task 3: Filename Parser

**File**: `ai-tutor-api/src/services/filename_parser.py` (NEW)

Token-scanning approach (not positional) to handle variable-length patterns.

```python
@dataclass
class FilenameMetadata:
    spec_code: str
    year: int | None = None
    session: str | None = None        # jun, nov, mar
    paper_number: str | None = None   # p1, p2, p3
    tier: str | None = None           # foundation, higher, core, extended
    doc_type: str | None = None       # qp, ms, er, gt, sp, spec, rev
    provider: str | None = None       # sme, pmt
    topic_slug: str | None = None     # for revision notes
    is_sample: bool = False
```

**Algorithm**:
1. Strip `.pdf`, split on `_`
2. Token 0 = spec_code
3. If token 1 == `"specification"` → return `{doc_type="spec"}`
4. If token 1 == `"sample"` → set `is_sample=True`, scan from token 2
5. Scan remaining tokens by matching against known sets:
   - `SESSION = {jun, nov, mar, jan}`
   - `TIER = {foundation, higher, core, extended}`
   - `DOC_TYPE = {qp, ms, er, gt, sp}`
   - `PAPER = /^p\d$/`
   - `YEAR = /^(19|20)\d{2}$/`
   - `PROVIDER = {sme, pmt}`
6. Unmatched tokens → accumulate into `topic_slug`

**Handles all patterns from the naming doc**:
- Tiered paper: `8461_2024_jun_p1_higher_qp.pdf`
- Untiered paper: `8145_2024_jun_p1_qp.pdf`
- Per-spec ER: `8300_2024_jun_er.pdf`
- Tiered ER: `8461_2024_nov_p1_foundation_er.pdf`
- Grade threshold: `8300_2024_jun_gt.pdf`
- Sample: `8300_sample_p1_foundation_qp.pdf`
- Spec: `8461_specification.pdf`
- Revision (SME): `8300_sme_01_number.pdf`
- Revision (PMT): `8463_pmt_01_energy.pdf`
- Insert: `8462_2024_jun_p1-insert_higher_qp.pdf` (handle `-insert` in paper token)

**Tests**: `ai-tutor-api/tests/test_filename_parser.py` (NEW) — one test per pattern above.

---

## Task 4: Extend Metadata Resolver

**File**: `ai-tutor-api/src/services/metadata_resolver.py`

### Subject identity model

The filename parser extracts `spec_code` (e.g., "8461") — a TEXT field on `public.subjects` that is effectively unique. The existing metadata resolver already uses `spec_code` to look up `subject_id` (UUID). Multiple rows in `subjects` may share a name (e.g., "Biology" at GCSE vs A-Level, "Biology A" vs "Biology B"), but only one has `spec_code = '8461'`.

- **Ingestion path**: filename → `spec_code` → `SELECT id FROM subjects WHERE spec_code = ?` → `subject_id` (UUID)
- **Search path**: user selects Qualification + Subject + Exam Board → frontend sends `subject_id` (UUID) → `rag.search_chunks(filter_subject_id := ?)`

This means the spec_code is an internal matching key only — never exposed to search APIs.

### New lookup functions

```python
@lru_cache(maxsize=256)
def _lookup_current_spec_version(subject_id: str) -> str | None:
    """Get current spec version for a subject from exam_spec_versions (WHERE is_current = true)."""

@lru_cache(maxsize=256)
def _lookup_pathway(subject_id: str, tier: str) -> str | None:
    """Map tier name (foundation/higher) to exam_pathway_id via exam_pathways table."""
```

Extend `ResolvedMetadata` dataclass:
```python
exam_spec_version_id: str | None = None
exam_pathway_id: str | None = None
```

Extend `resolve_metadata()` to:
1. Accept `FilenameMetadata` (session, paper, tier, doc_type)
2. Call `_lookup_current_spec_version(subject_id)` → set `exam_spec_version_id`
3. If tier is present, call `_lookup_pathway(subject_id, tier)` → set `exam_pathway_id`
4. Pass through session, paper_number, doc_type

---

## Task 5: Extend Ingestion Pipeline

### `ingestion.py`

Add new parameters to `ingest_document()`:
- `exam_spec_version_id`, `exam_pathway_id`, `session`, `paper_number`, `doc_type`, `file_key`

Store all in the `rag.documents` INSERT.

### `batch_ingestion.py`

Updated flow per file:
1. `parse_drive_path(path)` → folder metadata (existing)
2. `parse_filename(filename)` → filename metadata (NEW)
3. `resolve_metadata(path_meta, filename_meta)` → UUIDs (EXTENDED)
4. `download_file(file_id)` → bytes (existing)
5. `upload_to_storage(bytes, file_key)` → store original PDF (NEW)
6. `ingest_document(...)` → parse/chunk/embed/store (EXTENDED)

### Storage upload helper

New function (in `ingestion.py` or a small `storage.py`):
```python
def upload_to_storage(sb, file_bytes: bytes, file_key: str) -> None:
    sb.storage.from_("exam-documents").upload(file_key, file_bytes, {"content-type": "application/pdf"})
```

Build storage path from resolved metadata:
```python
f"{board_code.lower()}/{qual_code.lower()}/{spec_code.lower()}/{source_folder}/{year or ''}/{filename}"
```

---

## Task 6: Update Path Parser source_type mapping

**File**: `ai-tutor-api/src/services/path_parser.py`

The folder-level `source_type` should still be set from the folder name (`papers/` → `past_paper`, etc.), but now the filename parser provides `doc_type` which gives us finer granularity.

Map `doc_type` → `source_type` for cases where the folder-level mapping is too coarse:
- `qp` → `past_paper`
- `ms` → `marking_scheme`
- `er` → `examiner_report`
- `gt` → `grade_threshold`
- `sp` → `sample_paper` (new)
- `spec` → `specification`
- `sme`/`pmt` revision → `revision`

This means files in `papers/` that are mark schemes (`_ms.pdf`) get `source_type='marking_scheme'` instead of `'past_paper'` — more accurate than the current folder-only approach.

---

## Task 7: Tests

**New file**: `ai-tutor-api/tests/test_filename_parser.py`
- Test all filename patterns from the naming doc
- Test edge cases: hyphenated paper tokens (`p1-insert`), missing tokens, revision notes

**Update**: `ai-tutor-api/tests/conftest.py` — add mock for Storage client if needed

**Run**:
```bash
cd ai-tutor-api && ./venv/bin/python -m pytest tests/ -v
```

---

## Task 8: Single-Document Ingestion Test

After all schema + pipeline work is done:

1. **Verify lookup tables** — query `public.exam_boards` (AQA), `public.qualifications` (GCSE), `public.subjects` (8461)
2. **Run ingestion** on the Biology spec folder:
   ```bash
   cd ai-tutor-api && ./venv/bin/python scripts/ingest.py \
     --folder-id 1mu3zMtvUazledXYvwsscIr6vpE94xZdy \
     --label "test-biology-spec" --concurrency 1
   ```
3. **Verify**:
   - `rag.documents` row: status=completed, source_type=specification, doc_type=spec, file_key populated
   - `rag.chunks`: multiple chunks with embeddings (2000 dims)
   - Supabase Storage: original PDF accessible via signed URL
   - `rag.search_chunks()`: returns relevant Biology content for a Biology query
4. **Test dedup**: re-run, confirm it skips (content_hash match)

---

## Task 9: Documentation + Commit

- `CHANGELOG.md` — update Module 2 entry with schema alignment
- Conventional commits after each task
- Push to PR #46

---

## Key Design Decisions

| Decision | Choice | Why |
|----------|--------|-----|
| Tier column | FK to `exam_pathways`, not free text | Referential integrity, aligns with child pathway selection |
| `doc_type` + `source_type` | Keep both | `source_type` = user-facing category, `doc_type` = filename token |
| Storage bucket | Private `exam-documents` | Copyrighted materials, signed URL access only |
| Filename parser | Token-scanning, not positional | Handles variable-length patterns (tiered/untiered/revision) |
| Migration | ALTER TABLE (new migration file) | Safe for tables with or without existing data |
| Spec version | Default to `is_current=true` | Most docs relate to current spec; historical mapping later |
| Subject identity | `spec_code` for ingestion matching, `subject_id` for search | `spec_code` (e.g., "8461") is TEXT on `subjects` and effectively unique — filename parser extracts it, metadata resolver does `WHERE spec_code = ?` to get UUID. But users navigate via Qualification + Subject Name + Exam Board → `subject_id`, so search functions accept `subject_id` (UUID), never raw `spec_code`. Multiple subject rows may share a name (e.g., "Biology" at GCSE vs A-Level, "Biology A" vs "Biology B") but only one will have `spec_code = '8461'` |

---

## Files Created/Modified

| File | Action |
|------|--------|
| `supabase/migrations/20260225120000_rag_documents_schema_alignment.sql` | NEW — schema migration |
| `ai-tutor-api/src/services/filename_parser.py` | NEW — parse structured filenames |
| `ai-tutor-api/tests/test_filename_parser.py` | NEW — filename parser tests |
| `ai-tutor-api/src/services/metadata_resolver.py` | MODIFY — add spec_version + pathway lookup |
| `ai-tutor-api/src/services/ingestion.py` | MODIFY — new columns + Storage upload |
| `ai-tutor-api/src/services/batch_ingestion.py` | MODIFY — call filename parser + upload |
| `ai-tutor-api/src/services/path_parser.py` | MODIFY — use doc_type for source_type accuracy |
| `CHANGELOG.md` | MODIFY — update entry |

---

## Verification

1. `cd ai-tutor-api && ./venv/bin/python -m pytest tests/ -v` — all tests pass
2. `supabase db push --linked` — migration applied
3. Storage bucket created and accessible
4. Single-doc ingestion succeeds with all new metadata populated
5. `rag.search_chunks()` returns results with new return columns
6. Original PDF downloadable via signed URL
